# -*- coding: utf-8 -*-
"""ML-project-Midsem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ZWHyOneAZkxQ20PjolQZONx6pZglpVg
"""

import numpy as np
import pandas as pd
import nltk
#nltk.download('popular')
#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')
import re
import joblib
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn import metrics
import itertools
import datetime
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from wordcloud import WordCloud
from google.colab import drive 
drive.mount('/content/drive')

path = '/content/drive/My Drive/Colab Notebooks/MLProject/midsem/'
# dataset https://www.kaggle.com/c/fake-news/data
def get_dataframes():
    df=pd.read_csv(path+'train.csv').drop(['id'], axis=1).dropna()
    df['total'] = df['title'] + ' ' + df['text'] + ' ' + df['author']
    df.drop(['title', 'text', 'author'], axis=1, inplace=True)
    return df.sample(frac=1.0)
    
def get_corpus_tf(dataset, set_type):
  corpus = []
  wordnet=WordNetLemmatizer()
  for _, row in dataset.iterrows():
      review = re.sub('[^a-zA-Z]', ' ', row['total'])
      review = review.lower()
      review = review.split()
      review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]
      review = ' '.join(review)
      corpus.append(review)
  filename = path+'corpus{}'.format(set_type)
  joblib.dump(corpus, filename)
  return corpus

def get_corpus_bow(dataset, set_type):
  corpus = []
  ps = PorterStemmer()
  for _, row in dataset.iterrows():
      review = re.sub('[^a-zA-Z]', ' ', row['total'])
      review = review.lower()
      review = review.split()
      review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
      review = ' '.join(review)
      corpus.append(review)
  filename = path+'corpus{}'.format(set_type)
  joblib.dump(corpus, filename)
  return corpus


def word_cloud(data, label=0):
    wordcloud = WordCloud(width = 800, height = 800, 
                    background_color ='white', 
                    stopwords = stopwords.words('english'), 
                    min_font_size = 10).generate(" ".join(data[data['label'] == label].total)) 
    
    # plot the WordCloud image for genuine news data                     
    plt.figure(figsize = (8, 8), facecolor = None) 
    plt.imshow(wordcloud) 
    plt.axis("off") 
    plt.tight_layout(pad = 0) 
    plt.show() 

def corpus_prep_tf(train_corpus, test_corpus):
    tfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,3))
    X_train=tfidf_v.fit_transform(train_corpus).toarray()
    filename = path+'train-tf.sav'
    joblib.dump(X_train, filename)
    X_test=tfidf_v.transform(test_corpus).toarray()
    filename = path+'test-tf.sav'
    joblib.dump(X_test, filename)
    return X_train, X_test

def corpus_prep_bow(train_corpus, test_corpus):
    vectorizer = CountVectorizer()
    X_train=vectorizer.fit_transform(train_corpus).toarray()
    filename = path+'train-bag.sav'
    joblib.dump(X_train, filename)
    X_test=vectorizer.transform(train_corpus).toarray()
    filename = path+'test-bag.sav'
    joblib.dump(X_test, filename)
    return X_train, X_test


dataset = get_dataframes()
#word_cloud(dataset,1) #Fake News
#word_cloud(dataset,0) #Real News
X_train, X_test, y_train, y_test = train_test_split(dataset.drop(['label'], axis=1), dataset['label'])

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.get_cmap('Blues')):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print()
        #print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

def run_models(saved=True, c_type='-tf.sav'):
   
  def NB():
      filename = path+'NB{}'.format(c_type)
      if saved == True:
          classifier = joblib.load(filename)
      else:
          classifier=MultinomialNB()
          classifier.fit(X_train, y_train)
          joblib.dump(classifier, filename)
      pred = classifier.predict(X_test)
      score = metrics.accuracy_score(y_test, pred)
      print("accuracy:   %0.3f" % score)
      cm = metrics.confusion_matrix(y_test, pred)
      plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])


  def linear_svm():
      filename = path+'linear_svm{}'.format(c_type)
      if saved == True:
          classifier = joblib.load(filename)
      else:
          classifier=SVC(kernel='linear')
          classifier.fit(X_train, y_train)
          joblib.dump(classifier, filename)    
      pred = classifier.predict(X_test)
      score = metrics.accuracy_score(y_test, pred)
      print("accuracy:   %0.3f" % score)
      cm = metrics.confusion_matrix(y_test, pred)
      plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])


  def svm():
      filename = path+'svm{}'.format(c_type)
      if saved == True:
          classifier = joblib.load(filename)
      else:
          classifier=SVC()
          classifier.fit(X_train, y_train)
          joblib.dump(classifier, filename)
      pred = classifier.predict(X_test)
      score = metrics.accuracy_score(y_test, pred)
      print("accuracy:   %0.3f" % score)
      cm = metrics.confusion_matrix(y_test, pred)
      plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])


  def logistic_regression():
      filename = path+'logistic{}'.format(c_type)
      if saved == True:
          classifier = joblib.load(filename)
      else:
          classifier=LogisticRegression()
          classifier.fit(X_train, y_train)
          joblib.dump(classifier, filename)
      pred = classifier.predict(X_test)
      score = metrics.accuracy_score(y_test, pred)
      print("accuracy:   %0.3f" % score)
      cm = metrics.confusion_matrix(y_test, pred)
      plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])


  def decision_tree():
      filename = path+'decision_tree{}'.format(c_type)
      if saved == True:
          classifier = joblib.load(filename)
      else:
          classifier=DecisionTreeClassifier()
          classifier.fit(X_train, y_train)
          joblib.dump(classifier, filename)
      pred = classifier.predict(X_test)
      score = metrics.accuracy_score(y_test, pred)
      print("accuracy:   %0.3f" % score)
      cm = metrics.confusion_matrix(y_test, pred)
      plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])
    
  NB()
  logistic_regression()
  decision_tree()
  linear_svm()
  svm()

saved = True
c_type = '-bag.sav'
if saved == True:
  train_corpus = joblib.load(path+'corpus-train'+c_type)
  test_corpus = joblib.load(path+'corpus-test'+c_type)
  X_train, X_test = corpus_prep_bow(train_corpus, test_corpus)
 #X_train = joblib.load(path+'train'+c_type)
  #X_test = joblib.load(path+'test'+c_type)
else:
  train_corpus = get_corpus_bow(X_train, '-train'+c_type)
  test_corpus = get_corpus_bow(X_test, '-test'+c_type)
  X_train, X_test = corpus_prep_bow(train_corpus, test_corpus)

run_models(False, c_type)

saved = True
c_type = '-tf.sav'
if saved == True:
  X_train = joblib.load(path+'train'+c_type)
  X_test = joblib.load(path+'test'+c_type)
else:
  train_corpus = get_corpus_tf(X_train, 'train'+c_type)
  test_corpus = get_corpus_tf(X_test, 'test'+c_type)
  X_train, X_test = corpus_prep_tf(train_corpus, test_corpus)

run_models(saved, c_type)