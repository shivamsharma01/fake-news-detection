# -*- coding: utf-8 -*-
"""ML-project-LSTM+W2V+Attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gvqjxhZMXTL10ZV8Z6jbFgmNh1z1PwBG
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import re
import joblib
from numpy import array
from google.colab import drive 
drive.mount('/content/drive')

import gensim,keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Embedding, Activation, Dropout, SimpleRNN, LSTM, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import nltk
#nltk.download("popular")
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

stop = set(stopwords.words('english'))
MAX_SEQUENCE_LENGTH = 50
MAX_NB_WORDS = 100000
EMBEDDING_DIM = 50
num_lstm = 300
num_dense = 256
lstm_dropout_rate = 0.25
dense_dropout_rate = 0.25

act = 'relu'


size_embedding = 200
windows = 2
min_count = 1
maxlen = 1000

#reference https://kgptalkie.com/words-embedding-using-glove-vectors/
contractions = { 
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he'll've": "he will have",
"he's": "he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how does",
"i'd": "i would",
"i'd've": "i would have",
"i'll": "i will",
"i'll've": "i will have",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'd've": "it would have",
"it'll": "it will",
"it'll've": "it will have",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she would",
"she'd've": "she would have",
"she'll": "she will",
"she'll've": "she will have",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so is",
"that'd": "that would",
"that'd've": "that would have",
"that's": "that is",
"there'd": "there would",
"there'd've": "there would have",
"there's": "there is",
"they'd": "they would",
"they'd've": "they would have",
"they'll": "they will",
"they'll've": "they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
"weren't": "were not",
" u ": " you ",
" ur ": " your ",
" n ": " and "}

filename = '/content/drive/My Drive/Colab Notebooks/MLProject/'

wordnet=WordNetLemmatizer()

def clean_text(sentence):
  if type(sentence) is str:
    sentence = sentence.lower()
    for key in contractions:
      value = contractions[key]
      sentence = sentence.replace(key, value)
    sentence = re.sub('[^A-Z a-z]+', '', sentence)
    sentence = [wordnet.lemmatize(word) for word in sentence.split()]
    sentence = ' '.join(sentence)
    return sentence
  else:
    return sentence
  
def read_glove_vecs(glove_file):
  with open(glove_file, encoding="utf8") as f:
    words = set()
    word_to_vec_map = {}
    for line in f:
      line = line.strip().split()
      curr_word = line[0]
      words.add(curr_word)
      word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)
  return word_to_vec_map

def w2v_to_keras_weights(model, vocab):
    vocab_size = len(vocab) + 1
    weight_matrix = np.zeros((vocab_size, size_embedding))
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

pd.set_option('max_colwidth', 170)
dataframe=pd.read_csv(filename + 'train.csv').drop('id', axis=1).sample(frac=1)
# 'title' 'author' 'text' 'label'
dataframe.text=dataframe.text.astype(str)
print(dataframe)
print(dataframe.columns.values)
X_train, X_test, y_train, y_test = train_test_split(dataframe['text'], dataframe['label'], random_state = 42, test_size = 0.2, stratify = dataframe['label'])

X_train = X_train.apply(lambda x: clean_text(x))
X_test = X_test.apply(lambda x: clean_text(x))
print(X_train.head())

df = dataframe

text = X_train.tolist()
token = Tokenizer(num_words = MAX_NB_WORDS)
token.fit_on_texts(text)
vocab_size  = len(token.word_index) + 1
print(vocab_size)
encoded_text = token.texts_to_sequences(text)

max_length = len(max(encoded_text, key = lambda i: len(i)))
text = pad_sequences(encoded_text, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
print(len(text))
print(text[0])

test_sequences = token.texts_to_sequences(X_test)
X_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
word_index = token.word_index
nb_words  = min(MAX_NB_WORDS, len(word_index))

saved = True
if saved == True:
  embedding_matrix = joblib.load(filename+'w2v-matrix.sav')  
else:
  word_index = token.word_index
  text_train_splited = [article.split() for article in X_train.tolist()]
  w2v_model = gensim.models.Word2Vec(sentences = text_train_splited, size = size_embedding, window = windows, min_count = min_count)
  embedding_matrix = w2v_to_keras_weights(w2v_model, word_index)
  joblib.dump(embedding_matrix, filename+'w2v-matrix.sav')
  print(embedding_matrix.shape)

from tensorflow import keras
from keras import backend as K
from keras.engine.topology import Layer
from keras import initializers, regularizers, constraints

from keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed
from keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D
from keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate
from keras.layers import Reshape, merge, Concatenate, Lambda, Average
from keras.models import Sequential, Model
from keras.initializers import Constant
from keras.layers.merge import add

class Attention(keras.layers.Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')
        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)
        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)
        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3
        self.W = self.add_weight(shape=(input_shape[-1],),
                                 name='{}_W'.format(self.name),
                                 initializer=self.init,
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]
        if self.bias:
            self.b = self.add_weight(shape=(input_shape[1],),
                                     name='{}_b'.format(self.name),
                                     initializer='zero',
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None
        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim
        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))
        if self.bias:
            eij += self.b
        eij = K.tanh(eij)
        a = K.exp(eij)
        if mask is not None:
            a *= K.cast(mask, K.floatx())
        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0],  self.features_dim

def set_model(embedding_vectors):
  nb_words  = min(MAX_NB_WORDS, len(word_index))
  embedding_layer = Embedding(embedding_vectors.shape[0], output_dim=embedding_vectors.shape[1],weights=[embedding_vectors], input_length=maxlen, trainable=False)
  lstm_layer = LSTM(num_lstm, dropout = lstm_dropout_rate, recurrent_dropout = lstm_dropout_rate, return_sequences = True )       

  input_comment = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype = 'int32')
  embedded_sequence = embedding_layer(input_comment)
  x = lstm_layer(embedded_sequence)
  x = Dropout(dense_dropout_rate)(x)
  merged = Attention(MAX_SEQUENCE_LENGTH)(x)
  merged = Dense(64, activation = act)(merged)
  merged = Dropout(dense_dropout_rate)(merged)
  merged = BatchNormalization()(merged)
  preds = Dense(1, activation = 'sigmoid')(merged)
  model = Model(inputs = [input_comment], outputs = preds)
  model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])
  print(model.summary())
  return model

saved = False
filename = '/content/drive/My Drive/Colab Notebooks/MLProject/w2v'
csv_file = 'plot.csv'
if saved == True:
  model = tf.keras.models.load_model(filename)
  print(model.evaluate(X_train, y_train))
  print(model.evaluate(X_test, y_test))
  plot_df = pd.read_csv(filename+csv_file)
else:
  model = set_model(embedding_vectors = embedding_matrix)
  history = model.fit(text, y_train, epochs = 30, validation_data = (X_test, y_test))
  #model.save(filename)
  data = np.concatenate((
    np.array(history.history['accuracy']).reshape(-1,1),
    np.array(history.history['val_accuracy']).reshape(-1,1),
    np.array(history.history['loss']).reshape(-1,1),
    np.array(history.history['val_loss']).reshape(-1,1)), axis=1)
  plot_df = pd.DataFrame(data, columns=['Train Accuracy', 'Test Accuracy', 'Train Loss', 'Test Loss'])
  #plot_df.to_csv(filename+csv_file)

plt.figure(figsize = (10,10))
plt.plot(plot_df['Train Accuracy'])
plt.plot(plot_df['Test Accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.show()

plt.figure(figsize = (10,10))
plt.plot(plot_df['Train Loss'])
plt.plot(plot_df['Test Loss'])
plt.title('model Loss')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.show()